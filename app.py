import requests
import json
import os
import time
import csv
from datetime import datetime
from dotenv import load_dotenv
from urllib.parse import parse_qs, urlparse  # Add this import

# Load environment variables (make sure you have ACCESS_TOKEN=.env)
load_dotenv()
ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")

# After load_dotenv()
if not ACCESS_TOKEN:
    print("âŒ ERROR: ACCESS_TOKEN not found in .env file")
    exit(1)

# ===============================
# CONFIGURATION
# ===============================
COUNTRY = "DZ"             # Change to your target country (e.g. DZ)
SEARCH_TERM = "salviano"      # Must have search_terms or search_page_ids
LIMIT = 10                 # Number of ads to fetch per request (max 100)
GRAPH_API_VERSION = "v24.0"
GRAPH_API_URL = f"https://graph.facebook.com/{GRAPH_API_VERSION}/ads_archive"

print(f"ğŸ” Fetching ads from Facebook Ad Library (v24.0) for country: {COUNTRY} and keyword: '{SEARCH_TERM}' ...")

# ===============================
# API PARAMETERS
# ===============================
params = {
    "access_token": ACCESS_TOKEN,
    "ad_reached_countries": COUNTRY,
    "ad_active_status": "ALL",
    "ad_type": "ALL",
    "search_terms": SEARCH_TERM,
    "limit": LIMIT,
    # âœ… Supported fields in v24.0
    "fields": (
        "id,"
        "ad_creation_time,"
        "ad_creative_bodies,"
        "ad_creative_link_titles,"
        "ad_creative_link_descriptions,"
        "page_name,"
        "ad_snapshot_url,"
        "ad_reached_countries"
    ),
}

# ===============================
# API REQUEST
# ===============================
def fetch_all_ads(params):
    all_ads = []
    page_count = 0
    max_retries = 3
    
    try:
        while True:
            page_count += 1
            print(f"ğŸ“„ Fetching page {page_count}...", end='\r')
            
            # Add delay between requests
            if page_count > 1:
                time.sleep(1)
            
            # Stop after reasonable number of pages
            if page_count > 10:
                print("\nâš ï¸ Reached maximum number of pages (10)")
                break
                
            for attempt in range(max_retries):
                try:
                    # Add timeout to prevent hanging
                    response = requests.get(GRAPH_API_URL, params=params, timeout=10)
                    response.raise_for_status()
                    data = response.json()
                    break
                except requests.Timeout:
                    if attempt == max_retries - 1:
                        print("\nâŒ Request timed out after multiple attempts")
                        return all_ads
                    print(f"\nâš ï¸ Timeout occurred, retrying... (attempt {attempt + 1}/{max_retries})")
                    time.sleep(2)
                except requests.RequestException as e:
                    print(f"\nâŒ HTTP Error: {e}")
                    return all_ads
                except json.JSONDecodeError:
                    print("\nâŒ Invalid JSON response")
                    return all_ads
            
            if "error" in data:
                print("\nâŒ API Error:")
                print(json.dumps(data, indent=2))
                break
            
            current_ads = data.get("data", [])
            if not current_ads:
                break
            
            all_ads.extend(current_ads)
            print(f"âœ… Found {len(current_ads)} ads on page {page_count}      ")
            
            # Get next page if available
            paging = data.get("paging", {})
            if "next" not in paging:
                break
            
            # Fix: Parse the next URL properly
            next_url = paging["next"]
            parsed_url = urlparse(next_url)
            params = parse_qs(parsed_url.query)
            # Convert list values to single strings
            params = {k: v[0] for k, v in params.items()}
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Operation cancelled by user")
        return all_ads
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        return all_ads
    
    return all_ads

# ===============================
# DISPLAY RESULTS
# ===============================
def display_results(ads):
    print(f"âœ… Found {len(ads)} ads matching '{SEARCH_TERM}' in {COUNTRY}.\n")
    
    for i, ad in enumerate(ads, start=1):
        print(f"ğŸ”¹ [{i}] {ad.get('page_name', 'Unknown Page')}")
        print(f"ğŸ•’ Created: {ad.get('ad_creation_time')}")
        print(f"ğŸ“„ Title: {', '.join(ad.get('ad_creative_link_titles', [])) if ad.get('ad_creative_link_titles') else 'â€”'}")
        print(f"ğŸ’¬ Description: {', '.join(ad.get('ad_creative_link_descriptions', [])) if ad.get('ad_creative_link_descriptions') else 'â€”'}")
        print(f"ğŸ“¢ Body: {', '.join(ad.get('ad_creative_bodies', []))[:150] if ad.get('ad_creative_bodies') else 'â€”'}...")
        print(f"ğŸ”— Snapshot: {ad.get('ad_snapshot_url')}")
        print("-" * 80)
    
    print("\nâœ… Done.")

def save_results(ads):
    # Create 'results' directory if it doesn't exist
    if not os.path.exists('results'):
        os.makedirs('results')
    
    # Generate timestamp for unique filenames
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    base_filename = f"fb_ads_{COUNTRY}_{SEARCH_TERM}_{timestamp}"
    
    # Save as JSON
    json_path = f"results/{base_filename}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(ads, f, ensure_ascii=False, indent=2)
    print(f"âœ… Saved JSON results to: {json_path}")
    
    # Save as CSV
    csv_path = f"results/{base_filename}.csv"
    with open(csv_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        # Write headers
        headers = [
            'Page Name',
            'Creation Time',
            'Link Title',
            'Link Description',
            'Ad Body',
            'Snapshot URL'
        ]
        writer.writerow(headers)
        
        # Write data
        for ad in ads:
            row = [
                ad.get('page_name', ''),
                ad.get('ad_creation_time', ''),
                ', '.join(ad.get('ad_creative_link_titles', [])),
                ', '.join(ad.get('ad_creative_link_descriptions', [])),
                ', '.join(ad.get('ad_creative_bodies', [])),
                ad.get('ad_snapshot_url', '')
            ]
            writer.writerow(row)
    print(f"âœ… Saved CSV results to: {csv_path}")

def main():
    print(f"ğŸ” Fetching ads from Facebook Ad Library (v24.0) for country: {COUNTRY} and keyword: '{SEARCH_TERM}' ...")
    
    # Fetch ads
    all_ads = fetch_all_ads(params)
    
    # Error handling
    if isinstance(all_ads, dict) and "error" in all_ads:
        print("âŒ API Error:")
        print(json.dumps(all_ads, indent=2))
        return
    
    if not all_ads:
        print("âš ï¸ No ads found for this query.")
        return
        
    # Display results
    display_results(all_ads)
    
    # Save results
    save_results(all_ads)
    
    print("\nâœ… Done.")

if __name__ == "__main__":
    main()
